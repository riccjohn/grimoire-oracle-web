# Grimoire Oracle ‚Äî Web App Plan

This document captures the architectural decisions and implementation plan for Grimoire Oracle, a TTRPG rule-lookup chatbot built as a Next.js web application with a LangChain RAG pipeline.

---

## Stack

| Layer              | Choice                                 |
| ------------------ | -------------------------------------- |
| Chat LLM           | Google Gemini (`gemini-2.0-flash`)     |
| Embeddings         | Google (`gemini-embedding-001`, 3072-dim) |
| Vector Store       | Supabase pgvector                      |
| Keyword Search     | Supabase full-text search              |
| Hybrid Search      | `SupabaseHybridSearch` (SQL function)  |
| UI                 | Next.js 15 App Router                  |
| Frontend streaming | Vercel AI SDK (`useChat` hook)         |
| API key protection | Next.js API routes (server-side)       |
| Deployment         | GitLab CI ‚Üí Vercel (on push to `main`) |
| Ingestion trigger  | GitLab CI manual job                   |
| Hosting            | Vercel                                 |

### Stack rationale

**Google Gemini** ‚Äî Both the chat model (`gemini-2.0-flash`) and embeddings (`gemini-embedding-001`) come from a single Google AI Studio API key, which has a genuinely free tier with no expiry and no credit card required (15 RPM, 1M tokens/day ‚Äî more than sufficient for a side project). LangChain integration via `@langchain/google-genai`. The embedding model produces 3072-dimensional vectors. If you ever want to switch to a paid provider (e.g. Anthropic), it's a two-line swap in `lib/oracle-logic.ts`.

**Supabase** ‚Äî A single PostgreSQL database handles both semantic search (pgvector) and keyword search (PostgreSQL full-text search). LangChain's `SupabaseHybridSearch` retriever runs both in a single SQL function call, keeping all retrieval logic inside the database rather than in application code.

**Next.js SSR** ‚Äî API routes run on the server, so `GOOGLE_API_KEY` and Supabase credentials are never sent to the browser.

**Vercel AI SDK (`ai` package)** ‚Äî Used only on the frontend via the `useChat` React hook. It handles message history state, loading indicators, streaming text rendering, and form wiring ‚Äî eliminating the manual `useState` + `fetch` + `ReadableStream` boilerplate you'd otherwise write in `app/page.tsx`. On the server side, `LangChainAdapter.toDataStreamResponse()` converts LangChain's output stream into the format `useChat` expects. Critically, the AI SDK's `streamText`/`generateText` functions are **not** used ‚Äî LangChain stays in charge of the entire RAG pipeline. The AI SDK is purely a transport and UI convenience layer.

**GitLab CI** ‚Äî Two jobs in a single `.gitlab-ci.yml`: a `deploy` job that fires automatically on push to `main` (via Vercel CLI), and an `ingest-vault` job that only runs when manually triggered. This keeps deployment automated while preventing ingestion from running on every code change.

---

## Architecture

```
Browser
  ‚îÇ
  ‚îÇ  POST /api/chat   (streaming)
  ‚ñº
Next.js API Route  ‚Üê‚îÄ‚îÄ‚îÄ server-only, env vars protected here
  ‚îÇ
  ‚îú‚îÄ‚îÄ ChatGoogleGenerativeAI (gemini-2.0-flash)
  ‚îÇ
  ‚îî‚îÄ‚îÄ SupabaseHybridSearch retriever
        ‚îÇ
        ‚îú‚îÄ‚îÄ pgvector (semantic / cosine similarity)
        ‚îî‚îÄ‚îÄ PostgreSQL FTS (keyword search)
              ‚îÇ
              ‚îî‚îÄ‚îÄ Supabase (documents table)
                    ‚ñ≤
                    ‚îÇ  populated by:
                    ‚îÇ
              GitLab CI ‚Äî ingest-vault (manual trigger)
                    ‚îÇ
              scripts/ingest.ts
                    ‚îÇ
              ‚îú‚îÄ‚îÄ GoogleGenerativeAIEmbeddings (gemini-embedding-001)
              ‚îî‚îÄ‚îÄ vault/ markdown files
```

### Hybrid search

`SupabaseHybridSearch` runs a single PostgreSQL RPC function that performs both vector similarity search and full-text search inside the database, then returns a merged result. All hybrid logic lives in SQL ‚Äî fewer round trips, no orchestration in TypeScript.

---

## File Structure

```
grimoire-oracle-web/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ page.tsx              # Chat UI (client component)
‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx
‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ       ‚îî‚îÄ‚îÄ chat/
‚îÇ           ‚îî‚îÄ‚îÄ route.ts      # Streaming API route (server-only)
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îú‚îÄ‚îÄ oracle-logic.ts       # RAG pipeline (server-only)
‚îÇ   ‚îî‚îÄ‚îÄ supabase.ts           # Supabase client initialization
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ ingest.ts             # Ingestion script ‚Üí Supabase
‚îú‚îÄ‚îÄ vault/                    # TTRPG markdown files (or git submodule)
‚îú‚îÄ‚îÄ .gitlab-ci.yml            # CI/CD with manual ingest job
‚îú‚îÄ‚îÄ .env.local                # Local dev env vars (gitignored)
‚îú‚îÄ‚îÄ .env.example              # Template for required env vars
‚îú‚îÄ‚îÄ package.json
‚îî‚îÄ‚îÄ tsconfig.json
```

---

## Environment Variables

```bash
# .env.local (never commit this file)

# Runtime ‚Äî Next.js app (also set in Vercel project settings)
GOOGLE_API_KEY=              # Google AI Studio ‚Äî free tier, covers both LLM and embeddings
SUPABASE_URL=                # Found in Supabase project settings
SUPABASE_ANON_KEY=           # Safe for read queries (RLS enforced)

# Observability ‚Äî Next.js app (also set in Vercel project settings)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=           # From smith.langchain.com
LANGCHAIN_PROJECT=grimoire-oracle-web

# Ingestion only ‚Äî never set in Vercel; lives only in GitLab CI variables
SUPABASE_SERVICE_ROLE_KEY=   # Write access ‚Äî bypasses RLS
```

**Where each variable lives:**

| Variable | `.env.local` | Vercel settings | GitLab CI |
|---|---|---|---|
| `GOOGLE_API_KEY` | Yes | Yes | Yes (ingest) |
| `SUPABASE_URL` | Yes | Yes | No |
| `SUPABASE_ANON_KEY` | Yes | Yes | No |
| `LANGCHAIN_TRACING_V2` | Yes | Yes | No |
| `LANGCHAIN_API_KEY` | Yes | Yes | No |
| `LANGCHAIN_PROJECT` | Yes | Yes | No |
| `SUPABASE_SERVICE_ROLE_KEY` | Yes (local ingest) | **Never** | Yes |
| `VERCEL_TOKEN` | No | No | Yes |

`SUPABASE_SERVICE_ROLE_KEY` bypasses Row Level Security entirely ‚Äî if it were set in Vercel, a bug in the Next.js app could expose write access to the database. Keep it out of Vercel.

---

## GitLab CI/CD

Two jobs live in a single `.gitlab-ci.yml`. They appear as separate pipeline runs in the GitLab UI because they're triggered by different events: `deploy` fires on every push to `main`; `ingest-vault` only fires when you manually click "Run" in the GitLab Pipelines UI.

```yaml
# .gitlab-ci.yml

stages:
  - deploy
  - ingest

# Pipeline 1: Deploy chatbot to Vercel on every push to main
deploy-chatbot:
  stage: deploy
  image: node:20
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
  script:
    - pnpm install --frozen-lockfile
    - pnpm dlx vercel pull --yes --environment=production --token=$VERCEL_TOKEN
    - pnpm dlx vercel build --prod --token=$VERCEL_TOKEN
    - pnpm dlx vercel deploy --prebuilt --prod --token=$VERCEL_TOKEN
  variables:
    VERCEL_ORG_ID: $VERCEL_ORG_ID
    VERCEL_PROJECT_ID: $VERCEL_PROJECT_ID

# Pipeline 2: Rebuild the vector index on demand (never runs automatically)
ingest-vault:
  stage: ingest
  image: node:20
  when: manual
  rules:
    - when: manual
  script:
    - pnpm install --frozen-lockfile
    - pnpm tsx scripts/ingest.ts
  variables:
    SUPABASE_URL: $SUPABASE_URL
    SUPABASE_SERVICE_ROLE_KEY: $SUPABASE_SERVICE_ROLE_KEY
    GOOGLE_API_KEY: $GOOGLE_API_KEY
```

### CI/CD variables to configure (Settings ‚Üí CI/CD ‚Üí Variables)

| Variable | Used by | Mask? | Protect? |
|---|---|---|---|
| `VERCEL_TOKEN` | `deploy-chatbot` | Yes | Yes |
| `VERCEL_ORG_ID` | `deploy-chatbot` | No | No |
| `VERCEL_PROJECT_ID` | `deploy-chatbot` | No | No |
| `SUPABASE_URL` | `ingest-vault` | No | Yes |
| `SUPABASE_SERVICE_ROLE_KEY` | `ingest-vault` | Yes | Yes |
| `GOOGLE_API_KEY` | both jobs | Yes | Yes |

`SUPABASE_SERVICE_ROLE_KEY` bypasses Row Level Security ‚Äî keep it out of the `deploy-chatbot` job entirely. The deployed Next.js app uses the `SUPABASE_ANON_KEY` (set in Vercel project settings), which is scoped to read-only queries via RLS.

**Note on Vercel + GitLab:** Vercel has a native GitLab integration that can auto-deploy without a CI job ‚Äî but using the Vercel CLI in GitLab CI gives you more control over environment selection and deployment timing within your pipeline.

---

## Implementation Phases

### Phase 1 ‚Äî Supabase setup

1. Create a new Supabase project
2. Enable the `pgvector` extension: `Database ‚Üí Extensions ‚Üí vector`
3. LangChain's `SupabaseVectorStore` will auto-create the `documents` table on first ingest ‚Äî but you can also create it manually to understand the schema:
   ```sql
   create table documents (
     id bigserial primary key,
     content text,
     metadata jsonb,
     embedding vector(3072)    -- 3072 dims for Google gemini-embedding-001
   );
   ```
4. Create the hybrid search SQL function (see [Supabase LangChain docs](https://supabase.com/docs/guides/ai/langchain))

### Phase 2 ‚Äî Ingestion (`scripts/ingest.ts`)

The ingestion script reads markdown files from `vault/`, chunks and enriches them, embeds each chunk with `GoogleGenerativeAIEmbeddings`, and writes everything to the Supabase `documents` table via `SupabaseVectorStore.fromDocuments()`.

Key functions to implement:
- `splitDocsIntoChunks` ‚Äî load and chunk markdown files
- `mergeSmallChunks` ‚Äî combine fragments below a minimum token threshold
- `enrichChunksWithMetadata` ‚Äî attach source file, section title, etc.
- `SupabaseVectorStore.fromDocuments()` ‚Äî embed and store in one call

### Phase 3 ‚Äî Oracle logic (`lib/oracle-logic.ts`)

The RAG pipeline uses three LangChain primitives composed together:

- `createHistoryAwareRetriever` ‚Äî rephrases follow-up questions using chat history before running retrieval
- `createStuffDocumentsChain` ‚Äî stuffs retrieved documents into the prompt context
- `createRetrievalChain` ‚Äî orchestrates the full pipeline end-to-end

The retriever is `SupabaseHybridSearch`, which runs vector and full-text search in a single SQL call.

Mark the module server-only by keeping it in `lib/` and only importing it from API routes ‚Äî never from client components.

### Phase 4 ‚Äî Next.js UI

- `app/api/chat/route.ts` ‚Äî Streaming POST route. Calls `setupOracle()`, invokes `chain.stream()`, and uses `LangChainAdapter` to return a stream in the format `useChat` expects.
- `app/page.tsx` ‚Äî Chat interface built with the `useChat` hook. No manual streaming, state management, or fetch wiring needed.

API route skeleton:

```typescript
// app/api/chat/route.ts
import { LangChainAdapter } from 'ai';
import { setupOracle } from '@/lib/oracle-logic';
import { AIMessage, HumanMessage } from '@langchain/core/messages';
import type { Message } from 'ai';

export async function POST(req: Request) {
  const { messages }: { messages: Message[] } = await req.json();

  const chatHistory = messages.slice(0, -1).map((m) =>
    m.role === 'user' ? new HumanMessage(m.content) : new AIMessage(m.content),
  );
  const input = messages.at(-1)?.content ?? '';

  const chain = await setupOracle();
  const stream = await chain.stream({ input, chat_history: chatHistory });

  // LangChainAdapter bridges LangChain's output stream to the AI SDK data stream format
  return LangChainAdapter.toDataStreamResponse(stream, {
    inputKey: 'answer', // tells the adapter which key in the chunk holds the text
  });
}
```

Frontend skeleton:

```typescript
// app/page.tsx
'use client';
import { useChat } from 'ai/react';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit, isLoading } = useChat();

  return (
    <div>
      {messages.map((m) => (
        <div key={m.id}>
          <strong>{m.role === 'user' ? '‚ùØ' : 'üßô'}</strong> {m.content}
        </div>
      ))}
      {isLoading && <p>Consulting the grimoire...</p>}
      <form onSubmit={handleSubmit}>
        <input value={input} onChange={handleInputChange} placeholder="Ask me about OSE rules..." />
        <button type="submit">Ask</button>
      </form>
    </div>
  );
}
```

### Phase 5 ‚Äî GitLab CI/CD

- Add `.gitlab-ci.yml` to the repo root with both jobs (`deploy-chatbot` and `ingest-vault`)
- In Vercel: create a project, link it to the GitLab repo, grab the `VERCEL_TOKEN`, `VERCEL_ORG_ID`, and `VERCEL_PROJECT_ID`
- Add all CI/CD variables to GitLab (Settings ‚Üí CI/CD ‚Üí Variables) per the table in the CI/CD section above
- Add runtime variables to Vercel project settings (`GOOGLE_API_KEY`, `SUPABASE_URL`, `SUPABASE_ANON_KEY`, LangSmith vars)
- Push to `main` ‚Üí confirm `deploy-chatbot` runs automatically
- Manually trigger `ingest-vault` ‚Üí confirm ingestion runs and rows appear in Supabase

---

## Observability

### LLM tracing with LangSmith

LangSmith is LangChain's observability platform. Enabling it requires zero code changes ‚Äî just three environment variables. Every chain invocation is automatically traced.

**What it captures for each query:**
- The original user input
- The rephrased search query generated by `createHistoryAwareRetriever`
- The exact documents retrieved by `SupabaseHybridSearch` (with scores)
- The full assembled prompt sent to Gemini
- The model response and token counts
- End-to-end latency per step

This is the primary tool for debugging RAG quality issues. Failures in RAG systems fall into two buckets:
1. **Retrieval failure** ‚Äî the right documents weren't surfaced (fix: tune chunking, embedding model, or hybrid search weights)
2. **Generation failure** ‚Äî the right documents were retrieved but Gemini synthesized a bad answer (fix: tune the system prompt)

LangSmith lets you distinguish between them by inspecting actual traces ‚Äî you can see exactly which vault chunks were passed as `{context}` for any given query.

**Free tier:** 100K traces/month ‚Äî more than enough for a personal project.

**Setup:** Add to `.env.local` and Vercel project settings:
```bash
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=           # From smith.langchain.com
LANGCHAIN_PROJECT=grimoire-oracle-web
```

No code changes required. LangChain picks these up automatically.

### Vercel built-ins (zero setup)

Vercel provides request logs and basic analytics on the free plan with no configuration:
- **Functions tab** ‚Äî real-time logs for API route invocations, including cold start times
- **Analytics** ‚Äî page view counts (enable in Vercel dashboard, one click)

---

## Dependencies

```bash
pnpm add next react react-dom
pnpm add ai                          # Vercel AI SDK ‚Äî useChat hook + LangChainAdapter
pnpm add @langchain/google-genai @langchain/community @langchain/core @langchain/textsplitters
pnpm add @supabase/supabase-js
pnpm add -D typescript @types/node @types/react tsx
```

`@langchain/google-genai` covers both `ChatGoogleGenerativeAI` and `GoogleGenerativeAIEmbeddings` ‚Äî one package, one API key, no cost. The Supabase vector store comes via `@langchain/community`. `@ai-sdk/google` is deliberately **not** installed: the AI SDK's provider packages are only needed if you use `streamText`/`generateText`, which you're not ‚Äî LangChain handles all LLM and embedding calls.

---

## Verification Checklist

- [ ] Run `pnpm tsx scripts/ingest.ts` locally ‚Üí confirm rows appear in Supabase `documents` table
- [ ] Run `pnpm dev` ‚Üí open `http://localhost:3000` ‚Üí ask a rules question ‚Üí confirm streaming response
- [ ] Open LangSmith ‚Üí confirm a trace appeared ‚Üí inspect the retrieved documents and rephrased query
- [ ] Open browser DevTools ‚Üí Network tab ‚Üí inspect the `/api/chat` request ‚Üí confirm no API keys in request headers or response body
- [ ] Push to `main` ‚Üí confirm `deploy-chatbot` CI job runs automatically and Vercel deployment succeeds
- [ ] Trigger the `ingest-vault` job manually in GitLab UI ‚Üí confirm it completes and rows appear in Supabase `documents` table
- [ ] Confirm `SUPABASE_SERVICE_ROLE_KEY` is NOT present in Vercel project environment variables
- [ ] Visit the production Vercel URL ‚Üí confirm streaming chat works end-to-end
