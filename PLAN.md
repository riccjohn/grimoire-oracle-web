# Grimoire Oracle ‚Äî Web App Plan

This document captures the architectural decisions and implementation plan for Grimoire Oracle, a TTRPG rule-lookup chatbot built as a Next.js web application with a LangChain RAG pipeline.

---

## Stack

| Layer              | Choice                                 |
| ------------------ | -------------------------------------- |
| Chat LLM           | Google Gemini (`gemini-2.0-flash`)     |
| Embeddings         | Cohere (`embed-english-v3.0`, 1024-dim)  |
| Vector Store       | Supabase pgvector                      |
| Retrieval          | `SupabaseVectorStore` (vector search via `match_documents`) |
| Hybrid Search      | Deferred ‚Äî add `hybrid_search` RPC if retrieval quality is poor |
| UI                 | Next.js 15 App Router                  |
| Frontend streaming | Vercel AI SDK (`useChat` hook)         |
| API key protection | Next.js API routes (server-side)       |
| Deployment         | GitHub Actions ‚Üí Vercel (on push to `main`) |
| Ingestion trigger  | GitHub Actions manual workflow              |
| Hosting            | Vercel                                 |

### Stack rationale

**Google Gemini** ‚Äî The chat model (`gemini-2.0-flash`) uses a Google AI Studio API key, which has a genuinely free tier with no expiry and no credit card required (15 RPM, 1M tokens/day ‚Äî more than sufficient for a side project). LangChain integration via `@langchain/google-genai`.

**Cohere** ‚Äî Embeddings use Cohere's `embed-english-v3.0` model (1024-dimensional vectors) via `@langchain/cohere`. The free trial key allows ~1000 API calls/month, each processing up to 96 texts ‚Äî around 11 calls total for the full vault. This is far more practical than Google's embedding free tier (1000 single calls/day, batch endpoint unsupported). Requires a separate `COHERE_API_KEY`.

**Supabase** ‚Äî A single PostgreSQL database handles semantic search via pgvector. `SupabaseVectorStore` from `@langchain/community` runs vector similarity queries against the `match_documents` RPC. Full-text hybrid search is deferred ‚Äî the `hybrid_search` RPC pattern (RRF combining vector + FTS) can be added to the schema if retrieval quality needs improvement.

**Next.js SSR** ‚Äî API routes run on the server, so `GOOGLE_API_KEY` and Supabase credentials are never sent to the browser.

**Vercel AI SDK (`ai` package)** ‚Äî Used only on the frontend via the `useChat` React hook. It handles message history state, loading indicators, streaming text rendering, and form wiring ‚Äî eliminating the manual `useState` + `fetch` + `ReadableStream` boilerplate you'd otherwise write in `app/page.tsx`. On the server side, `LangChainAdapter.toDataStreamResponse()` converts LangChain's output stream into the format `useChat` expects. Critically, the AI SDK's `streamText`/`generateText` functions are **not** used ‚Äî LangChain stays in charge of the entire RAG pipeline. The AI SDK is purely a transport and UI convenience layer.

**GitHub Actions** ‚Äî Two workflows in `.github/workflows/`: a `deploy` workflow that fires automatically on push to `main` (via Vercel CLI), and an `ingest.yml` workflow that only runs when manually triggered via `workflow_dispatch`. This keeps deployment automated while preventing ingestion from running on every code change.

---

## Architecture

```
Browser
  ‚îÇ
  ‚îÇ  POST /api/chat   (streaming)
  ‚ñº
Next.js API Route  ‚Üê‚îÄ‚îÄ‚îÄ server-only, env vars protected here
  ‚îÇ
  ‚îú‚îÄ‚îÄ ChatGoogleGenerativeAI (gemini-2.0-flash)
  ‚îÇ
  ‚îî‚îÄ‚îÄ SupabaseVectorStore retriever
        ‚îÇ
        ‚îî‚îÄ‚îÄ pgvector (semantic / cosine similarity)
              ‚îÇ
              ‚îî‚îÄ‚îÄ Supabase (documents table)
                    ‚ñ≤
                    ‚îÇ  populated by:
                    ‚îÇ
              GitHub Actions ‚Äî ingest-vault (manual trigger)
                    ‚îÇ
              scripts/ingest.ts
                    ‚îÇ
              ‚îú‚îÄ‚îÄ CohereEmbeddings (embed-english-v3.0)
              ‚îî‚îÄ‚îÄ vault/ markdown files
```

### Retrieval

`SupabaseVectorStore` queries the `match_documents` RPC for vector similarity search. If retrieval quality is poor (e.g. exact OSE terms like spell names aren't surfacing), the upgrade path is to add a `hybrid_search` RPC to the schema that combines vector + full-text search via Reciprocal Rank Fusion (RRF), then pass `queryName: "hybrid_search"` to the vector store.

---

## File Structure

```
grimoire-oracle-web/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ page.tsx              # Chat UI (client component)
‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx
‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ       ‚îî‚îÄ‚îÄ chat/
‚îÇ           ‚îî‚îÄ‚îÄ route.ts      # Streaming API route (server-only)
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îú‚îÄ‚îÄ oracle-logic.ts       # RAG pipeline (server-only)
‚îÇ   ‚îî‚îÄ‚îÄ supabase-client.ts    # Supabase client initialization
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ ingest.ts             # Ingestion script ‚Üí Supabase
‚îú‚îÄ‚îÄ vault/                    # TTRPG markdown files (or git submodule)
‚îú‚îÄ‚îÄ .github/workflows/
‚îÇ   ‚îî‚îÄ‚îÄ ingest.yml            # Manual ingestion trigger (workflow_dispatch)
‚îú‚îÄ‚îÄ .env.local                # Local dev env vars (gitignored)
‚îú‚îÄ‚îÄ .env.example              # Template for required env vars
‚îú‚îÄ‚îÄ package.json
‚îî‚îÄ‚îÄ tsconfig.json
```

---

## Environment Variables

```bash
# .env.local (never commit this file)

# Runtime ‚Äî Next.js app (also set in Vercel project settings)
GOOGLE_API_KEY=              # Google AI Studio ‚Äî chat LLM only (gemini-2.0-flash)
COHERE_API_KEY=              # Cohere ‚Äî embeddings only (embed-english-v3.0)
SUPABASE_URL=                # Found in Supabase project settings
SUPABASE_ANON_KEY=           # Safe for read queries (RLS enforced)

# Observability ‚Äî Next.js app (also set in Vercel project settings)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=           # From smith.langchain.com
LANGCHAIN_PROJECT=grimoire-oracle-web

# Ingestion only ‚Äî never set in Vercel; lives only in GitLab CI variables
SUPABASE_SERVICE_ROLE_KEY=   # Write access ‚Äî bypasses RLS
```

**Where each variable lives:**

| Variable | `.env.local` | Vercel settings | GitHub Actions |
|---|---|---|---|
| `GOOGLE_API_KEY` | Yes | Yes | No |
| `COHERE_API_KEY` | Yes | No | Yes (ingest) |
| `SUPABASE_URL` | Yes | Yes | No |
| `SUPABASE_ANON_KEY` | Yes | Yes | No |
| `LANGCHAIN_TRACING_V2` | Yes | Yes | No |
| `LANGCHAIN_API_KEY` | Yes | Yes | No |
| `LANGCHAIN_PROJECT` | Yes | Yes | No |
| `SUPABASE_SERVICE_ROLE_KEY` | Yes (local ingest) | **Never** | Yes |
| `VERCEL_TOKEN` | No | No | Yes |

`SUPABASE_SERVICE_ROLE_KEY` bypasses Row Level Security entirely ‚Äî if it were set in Vercel, a bug in the Next.js app could expose write access to the database. Keep it out of Vercel.

---

## GitHub Actions CI/CD

One workflow in `.github/workflows/ingest.yml` handles manual ingestion. It only runs when triggered via `workflow_dispatch` (Actions tab ‚Üí "Ingest vault" ‚Üí "Run workflow") ‚Äî never automatically.

```yaml
# .github/workflows/ingest.yml
name: Ingest vault

on:
  workflow_dispatch:

jobs:
  ingest:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: pnpm/action-setup@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: pnpm
      - name: Install dependencies
        run: pnpm install --frozen-lockfile
      - name: Run ingestion pipeline
        run: pnpm tsx scripts/ingest.ts
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}
```

### Secrets to configure (Settings ‚Üí Secrets and variables ‚Üí Actions)

| Secret | Notes |
|---|---|
| `SUPABASE_URL` | Supabase project REST URL |
| `SUPABASE_SERVICE_ROLE_KEY` | Bypasses RLS ‚Äî keep out of Vercel |
| `COHERE_API_KEY` | Embedding model credentials |

`SUPABASE_SERVICE_ROLE_KEY` bypasses Row Level Security ‚Äî keep it out of Vercel entirely. The deployed Next.js app uses `SUPABASE_ANON_KEY` (set in Vercel project settings), scoped to read-only queries via RLS.

---

## Implementation Phases

### Phase 1 ‚Äî Supabase setup

1. Create a new Supabase project
2. Enable the `pgvector` extension: `Database ‚Üí Extensions ‚Üí vector`
3. LangChain's `SupabaseVectorStore` will auto-create the `documents` table on first ingest ‚Äî but you can also create it manually to understand the schema:
   ```sql
   create table documents (
     id bigserial primary key,
     content text,
     metadata jsonb,
     embedding vector(1024),   -- 1024 dims for Cohere embed-english-v3.0
     content_hash text unique
   );
   ```
4. Create the hybrid search SQL function (see [Supabase LangChain docs](https://supabase.com/docs/guides/ai/langchain))

### Phase 2 ‚Äî Ingestion (`scripts/ingest.ts`)

The ingestion script reads markdown files from `vault/`, chunks and enriches them, embeds each chunk with `CohereEmbeddings`, and writes everything to the Supabase `documents` table via `SupabaseVectorStore.fromDocuments()`.

Key functions to implement:
- `splitDocsIntoChunks` ‚Äî load and chunk markdown files
- `mergeSmallChunks` ‚Äî combine fragments below a minimum token threshold
- `enrichChunksWithMetadata` ‚Äî attach source file, section title, etc.
- `SupabaseVectorStore.fromDocuments()` ‚Äî embed and store in one call

### Phase 3 ‚Äî Oracle logic (`lib/oracle-logic.ts`)

The RAG pipeline uses three LangChain primitives composed together:

- `createHistoryAwareRetriever` ‚Äî rephrases follow-up questions using chat history before running retrieval
- `createStuffDocumentsChain` ‚Äî stuffs retrieved documents into the prompt context
- `createRetrievalChain` ‚Äî orchestrates the full pipeline end-to-end

The retriever is `SupabaseVectorStore` calling the `match_documents` RPC for vector similarity search.

Mark the module server-only by keeping it in `lib/` and only importing it from API routes ‚Äî never from client components.

### Phase 4 ‚Äî Next.js UI

- `app/api/chat/route.ts` ‚Äî Streaming POST route. Calls `setupOracle()`, invokes `chain.stream()`, and uses `LangChainAdapter` to return a stream in the format `useChat` expects.
- `app/page.tsx` ‚Äî Chat interface built with the `useChat` hook. No manual streaming, state management, or fetch wiring needed.

API route skeleton:

```typescript
// app/api/chat/route.ts
import { LangChainAdapter } from 'ai';
import { setupOracle } from '@/lib/oracle-logic';
import { AIMessage, HumanMessage } from '@langchain/core/messages';
import type { Message } from 'ai';

export async function POST(req: Request) {
  const { messages }: { messages: Message[] } = await req.json();

  const chatHistory = messages.slice(0, -1).map((m) =>
    m.role === 'user' ? new HumanMessage(m.content) : new AIMessage(m.content),
  );
  const input = messages.at(-1)?.content ?? '';

  const chain = await setupOracle();
  const stream = await chain.stream({ input, chat_history: chatHistory });

  // LangChainAdapter bridges LangChain's output stream to the AI SDK data stream format
  return LangChainAdapter.toDataStreamResponse(stream, {
    inputKey: 'answer', // tells the adapter which key in the chunk holds the text
  });
}
```

Frontend skeleton:

```typescript
// app/page.tsx ‚Äî AI SDK v6 API
'use client';
import { useChat } from '@ai-sdk/react';

export default function Page() {
  // v6 API ‚Äî old fields (input, handleInputChange, handleSubmit, isLoading) are gone
  const { messages, sendMessage, status } = useChat({ api: '/api/chat' });

  return (
    <div>
      {messages.map((m) => (
        <div key={m.id}>
          <strong>{m.role === 'user' ? '‚ùØ' : 'üßô'}</strong>{' '}
          {m.parts[0].type === 'text' ? m.parts[0].text : ''}
        </div>
      ))}
      {status === 'streaming' && <p>Consulting the grimoire...</p>}
    </div>
  );
}
```

### Phase 5 ‚Äî GitHub Actions CI/CD

- Add `.github/workflows/ingest.yml` with `workflow_dispatch` trigger
- Add `SUPABASE_URL`, `SUPABASE_SERVICE_ROLE_KEY`, and `COHERE_API_KEY` as repository secrets (Settings ‚Üí Secrets and variables ‚Üí Actions)
- Manually trigger the workflow from the Actions tab ‚Üí confirm ingestion runs and rows appear in Supabase

---

## Observability

### LLM tracing with LangSmith

LangSmith is LangChain's observability platform. Enabling it requires zero code changes ‚Äî just three environment variables. Every chain invocation is automatically traced.

**What it captures for each query:**
- The original user input
- The rephrased search query generated by `createHistoryAwareRetriever`
- The exact documents retrieved by `SupabaseVectorStore` (with scores)
- The full assembled prompt sent to Gemini
- The model response and token counts
- End-to-end latency per step

This is the primary tool for debugging RAG quality issues. Failures in RAG systems fall into two buckets:
1. **Retrieval failure** ‚Äî the right documents weren't surfaced (fix: tune chunking, embedding model, or hybrid search weights)
2. **Generation failure** ‚Äî the right documents were retrieved but Gemini synthesized a bad answer (fix: tune the system prompt)

LangSmith lets you distinguish between them by inspecting actual traces ‚Äî you can see exactly which vault chunks were passed as `{context}` for any given query.

**Free tier:** 100K traces/month ‚Äî more than enough for a personal project.

**Setup:** Add to `.env.local` and Vercel project settings:
```bash
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=           # From smith.langchain.com
LANGCHAIN_PROJECT=grimoire-oracle-web
```

No code changes required. LangChain picks these up automatically.

### Vercel built-ins (zero setup)

Vercel provides request logs and basic analytics on the free plan with no configuration:
- **Functions tab** ‚Äî real-time logs for API route invocations, including cold start times
- **Analytics** ‚Äî page view counts (enable in Vercel dashboard, one click)

---

## Dependencies

```bash
pnpm add next react react-dom
pnpm add ai                          # Vercel AI SDK ‚Äî useChat hook + LangChainAdapter
pnpm add @langchain/google-genai @langchain/cohere @langchain/community @langchain/core @langchain/textsplitters
pnpm add @supabase/supabase-js
pnpm add -D typescript @types/node @types/react tsx
```

`@langchain/google-genai` covers `ChatGoogleGenerativeAI` for the chat LLM. `@langchain/cohere` provides `CohereEmbeddings` for the embedding pipeline ‚Äî requires a separate `COHERE_API_KEY`. The Supabase vector store comes via `@langchain/community`. `@ai-sdk/google` is deliberately **not** installed: the AI SDK's provider packages are only needed if you use `streamText`/`generateText`, which you're not ‚Äî LangChain handles all LLM and embedding calls.

---

## Verification Checklist

- [ ] Run `pnpm tsx scripts/ingest.ts` locally ‚Üí confirm rows appear in Supabase `documents` table
- [ ] Run `pnpm dev` ‚Üí open `http://localhost:3000` ‚Üí ask a rules question ‚Üí confirm streaming response
- [ ] Open LangSmith ‚Üí confirm a trace appeared ‚Üí inspect the retrieved documents and rephrased query
- [ ] Open browser DevTools ‚Üí Network tab ‚Üí inspect the `/api/chat` request ‚Üí confirm no API keys in request headers or response body
- [ ] Push to `main` ‚Üí confirm Vercel deployment succeeds
- [ ] Trigger the `Ingest vault` workflow manually in GitHub Actions ‚Üí confirm it completes and rows appear in Supabase `documents` table
- [ ] Confirm `SUPABASE_SERVICE_ROLE_KEY` is NOT present in Vercel project environment variables
- [ ] Visit the production Vercel URL ‚Üí confirm streaming chat works end-to-end
